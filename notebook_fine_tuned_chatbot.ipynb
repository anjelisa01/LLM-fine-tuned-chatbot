{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMU1ilMzd5HFWUGZ4OviBAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjelisa01/LLM-fine-tuned-chatbot/blob/main/notebook_fine_tuned_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#clone repo"
      ],
      "metadata": {
        "id": "cIemUDfJPJqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OLjVIhHKAXES",
        "outputId": "c4e1266c-a4ff-4bd7-e307-d328678dafae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM-exploration'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 16 (delta 6), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (16/16), 22.24 KiB | 11.12 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "/content/LLM-exploration\n"
          ]
        }
      ],
      "source": [
        "#Up di github\n",
        "token = \"\"  #fill the token, delete after successfully clone the repo\n",
        "username = \"anjelisa01\"\n",
        "repo = \"LLM-fine-tuned-chatbot\"\n",
        "\n",
        "!git clone https://{username}:{token}@github.com/{username}/{repo}.git\n",
        "%cd {repo}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#installs and imports"
      ],
      "metadata": {
        "id": "5J-9lahzPOZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  transformers==4.38.2 \\\n",
        "  peft==0.8.2 \\\n",
        "  datasets \\\n",
        "  \"accelerate>=0.27.2,<0.28.0\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "l_iWLyyYPIpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "4uOk15lSCbcH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "FCYUXHvnCgBA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load model and tokenizer"
      ],
      "metadata": {
        "id": "h_MsaqcMPV4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "choosing this model because its lightweight and compatible with Google colab"
      ],
      "metadata": {
        "id": "5WzSsTmfPbcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "2rY48J8LCf3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tuning"
      ],
      "metadata": {
        "id": "5qb2cNmCPkzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##define lora config and apply to base model"
      ],
      "metadata": {
        "id": "fqxvAzONPv_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "02mXW1liCfv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##the training data"
      ],
      "metadata": {
        "id": "7Zk1qPnfQBaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"text\": [\n",
        "        \"### Human: What is the capital of France?\\n### Assistant: Paris.\",\n",
        "        \"### Human: Who wrote '1984'?\\n### Assistant: George Orwell.\",\n",
        "        \"### Human: What is the boiling point of water?\\n### Assistant: 100 degrees Celsius.\",\n",
        "        \"### Human: What's the square root of 64?\\n### Assistant: 8.\",\n",
        "        \"### Human: Who painted the Mona Lisa?\\n### Assistant: Leonardo da Vinci.\",\n",
        "        \"### Human: Whatâ€™s the largest planet in our solar system?\\n### Assistant: Jupiter.\",\n",
        "        \"### Human: When did World War II end?\\n### Assistant: 1945.\",\n",
        "        \"### Human: What is the chemical symbol for gold?\\n### Assistant: Au.\",\n",
        "        \"### Human: What does DNA stand for?\\n### Assistant: Deoxyribonucleic acid.\",\n",
        "        \"### Human: Who discovered gravity?\\n### Assistant: Isaac Newton.\"\n",
        "    ]\n",
        "}\n",
        "dataset = Dataset.from_dict(data)"
      ],
      "metadata": {
        "id": "kvt7KbXlQIoX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenized the training dataset"
      ],
      "metadata": {
        "id": "EIeVUlkyQPIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "dataset = dataset.map(tokenize)"
      ],
      "metadata": {
        "id": "1njcmU0iQIhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##setting training arguments"
      ],
      "metadata": {
        "id": "3-aHFwmCQVu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gptneo-lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=5, #dari 1 ganti ke 5\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
      ],
      "metadata": {
        "id": "lmPFJ2t2QIZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##training the model"
      ],
      "metadata": {
        "id": "ZAGFywipQf5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the base model using the datasets with the training arguments"
      ],
      "metadata": {
        "id": "35mlIGc1Qiki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "V-t-ipQzQISh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save the fine-tuned model"
      ],
      "metadata": {
        "id": "cQ_fFu6CQs7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we dont save the entire model only the configurations, so when we load the base model somewhere else we can apply this configuration"
      ],
      "metadata": {
        "id": "dt_Fi0RLQwza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gptneo-lora\")\n",
        "tokenizer.save_pretrained(\"./gptneo-lora\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j1yb5sG1QHyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#download the model"
      ],
      "metadata": {
        "id": "jbRjmQMSRGcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip it\n",
        "shutil.make_archive(\"gptneo-lora\", 'zip', \"gptneo-lora\")\n",
        "\n",
        "# Download\n",
        "files.download(\"gptneo-lora.zip\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BBdIjpYORJS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the model"
      ],
      "metadata": {
        "id": "qVW1hi9eVrk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will be in app.py"
      ],
      "metadata": {
        "id": "T7OVfXAOVuov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##function ask model"
      ],
      "metadata": {
        "id": "0oRu2c3BV5qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_model(prompt, model=model, tokenizer=tokenizer, max_new_tokens=20):\n",
        "    \"\"\"\n",
        "    Generate an answer from the fine-tuned model based on a custom prompt.\n",
        "    Args:\n",
        "        prompt (str): Your custom question or instruction.\n",
        "        model: Your fine-tuned Hugging Face model.\n",
        "        tokenizer: The tokenizer used with the model.\n",
        "        max_new_tokens (int): Max number of tokens to generate (default 20).\n",
        "    Returns:\n",
        "        str: Cleaned assistant response.\n",
        "    \"\"\"\n",
        "    # Format prompt like the training data\n",
        "    full_prompt = f\"### Human: {prompt}\\n### Assistant:\"\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    # Generate response\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    # Decode and clean\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant answer\n",
        "    if \"### Assistant:\" in decoded:\n",
        "        answer = decoded.split(\"### Assistant:\")[1].strip()\n",
        "        # Cut off hallucinated continuation (like another ### block or file paths)\n",
        "        for stop_token in [\"### Human:\", \"###\", \"\\n#\", \"\\n##\"]:\n",
        "            if stop_token in answer:\n",
        "                answer = answer.split(stop_token)[0].strip()\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "99CjYRT5Vzel"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##use the model"
      ],
      "metadata": {
        "id": "CfK346YLWPqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask_model(\"what is an influencer?\", model, tokenizer)\n",
        "print(\"ðŸ¤–\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUUaUcRuWNVU",
        "outputId": "156d9b78-5320-4dff-f4e6-11d17714ae00"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– a person who is influential in a certain field.\n"
          ]
        }
      ]
    }
  ]
}